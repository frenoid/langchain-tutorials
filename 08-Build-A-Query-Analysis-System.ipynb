{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "543efb70-8cc9-4d54-94d3-7a172870c466",
   "metadata": {},
   "source": [
    "# Build a Query Analysis System\n",
    "This page will show how to use query analysis in a basic end-to-end example. This will cover creating a simple search engine, showing a failure mode that occurs when passing a raw user question to that search, and then an example of how query analysis can help address that issue. There are MANY different query analysis techniques and this end-to-end example will not show all of them.\n",
    "\n",
    "For the purpose of this example, we will do retrieval over the LangChain YouTube videos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6aa6ff-b976-4647-b801-ef4de5f983e1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup\n",
    "Here are is an non-comprehensive list of dependencies used\n",
    "```bash\n",
    "langchain==0.2.6\n",
    "langchain-chroma==0.1.1\n",
    "langchain-community==0.2.6\n",
    "langchain-core==0.2.10\n",
    "langchain-openai==0.1.10\n",
    "langchain-text-splitters==0.2.0\n",
    "langchainhub==0.1.20\n",
    "langgraph==0.0.60\n",
    "langserve==0.2.1\n",
    "langsmith==0.1.81\n",
    "pytube==15.0.0\n",
    "youtube-transcript-api==0.6.2\n",
    "```\n",
    "Set environment variables\n",
    "We'll use OpenAI in this example:\n",
    "\n",
    "### Setting credentials with python-dot-env\n",
    "Load credentials from a `.env` file and the [python-dotenv package](https://pypi.org/project/python-dotenv/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4e533df-7b81-4bea-9c29-913e63cb2a6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "\n",
    "load_dotenv()\n",
    "assert os.environ[\"LANGCHAIN_API_KEY\"]\n",
    "assert os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0fc44c-24a1-4667-b312-ed4c9b527b32",
   "metadata": {},
   "source": [
    "## Load documents\n",
    "We can use the `YouTubeLoader` to load transcripts of a few LangChain videos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf7a1ce7-0c8e-4e84-be86-973381185239",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import YoutubeLoader\n",
    "\n",
    "urls = [\n",
    "    \"https://www.youtube.com/watch?v=HAn9vnJy6S4\",\n",
    "    \"https://www.youtube.com/watch?v=dA1cHGACXCo\",\n",
    "    \"https://www.youtube.com/watch?v=ZcEMLz27sL4\",\n",
    "    \"https://www.youtube.com/watch?v=hvAPnpSfSGo\",\n",
    "    \"https://www.youtube.com/watch?v=EhlPDL4QrWY\",\n",
    "    \"https://www.youtube.com/watch?v=mmBo8nlu2j0\",\n",
    "    \"https://www.youtube.com/watch?v=rQdibOsL1ps\",\n",
    "    \"https://www.youtube.com/watch?v=28lC4fqukoc\",\n",
    "    \"https://www.youtube.com/watch?v=es-9MgxB-uc\",\n",
    "    \"https://www.youtube.com/watch?v=wLRHwKuKvOE\",\n",
    "    \"https://www.youtube.com/watch?v=ObIltMaRJvY\",\n",
    "    \"https://www.youtube.com/watch?v=DjuXACWYkkU\",\n",
    "    \"https://www.youtube.com/watch?v=o7C9ld6Ln-M\",\n",
    "]\n",
    "docs = []\n",
    "\n",
    "# Pass the URLs to the YoutubeLoader which produce Documents to be added to to the list\n",
    "for url in urls:\n",
    "    docs.extend(\n",
    "        YoutubeLoader.from_youtube_url(url,\n",
    "                                       add_video_info=True).load()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516f7d42-c385-44e8-bc35-89ca8afa7866",
   "metadata": {},
   "source": [
    "__API Reference__: [YoutubeLoader](https://api.python.langchain.com/en/latest/document_loaders/langchain_community.document_loaders.youtube.YoutubeLoader.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a33d741b-1ea0-4d88-877a-b25ddefa8099",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# Add some additional metadata: what year the video was published\n",
    "for doc in docs:\n",
    "    doc.metadata[\"publish_year\"] = int(\n",
    "        datetime.datetime.strptime(\n",
    "            doc.metadata[\"publish_date\"], \"%Y-%m-%d %H:%M:%S\" ## Extract year from the doc's metadata\n",
    "        ).strftime(\"%Y\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b77a6d-5caf-4d09-8826-3fa5b9fb15b3",
   "metadata": {},
   "source": [
    "Here are the titles of the videos we've loaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4928e30-5660-43d9-80f5-70eb11a289c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OpenGPTs',\n",
       " 'Building a web RAG chatbot: using LangChain, Exa (prev. Metaphor), LangSmith, and Hosted Langserve',\n",
       " 'Streaming Events: Introducing a new `stream_events` method',\n",
       " 'LangGraph: Multi-Agent Workflows',\n",
       " 'Build and Deploy a RAG app with Pinecone Serverless',\n",
       " 'Auto-Prompt Builder (with Hosted LangServe)',\n",
       " 'Build a Full Stack RAG App With TypeScript',\n",
       " 'Getting Started with Multi-Modal LLMs',\n",
       " 'SQL Research Assistant',\n",
       " 'Skeleton-of-Thought: Building a New Template from Scratch',\n",
       " 'Benchmarking RAG over LangChain Docs',\n",
       " 'Building a Research Assistant from Scratch',\n",
       " 'LangServe and LangChain Templates Webinar']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[doc.metadata[\"title\"] for doc in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c29777b-762b-4783-9137-e547560a66c1",
   "metadata": {},
   "source": [
    "Here's the metadata associated with each video. We can see that each document also has a title, view count, publication date, and length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33ca3c73-e2da-484a-b418-c22500d95d3c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'HAn9vnJy6S4',\n",
       " 'title': 'OpenGPTs',\n",
       " 'description': 'Unknown',\n",
       " 'view_count': 8957,\n",
       " 'thumbnail_url': 'https://i.ytimg.com/vi/HAn9vnJy6S4/hq720.jpg',\n",
       " 'publish_date': '2024-01-31 00:00:00',\n",
       " 'length': 1530,\n",
       " 'author': 'LangChain',\n",
       " 'publish_year': 2024}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340af540-8262-4b27-9bd8-63ba98e0aee8",
   "metadata": {},
   "source": [
    "And here's a sample from a document's contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b9c23d7-9879-4a19-884f-c4b2bc43b459",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"hello today I want to talk about open gpts open gpts is a project that we built here at linkchain uh that replicates the GPT store in a few ways so it creates uh end user-facing friendly interface to create different Bots and these Bots can have access to different tools and they can uh be given files to retrieve things over and basically it's a way to create a variety of bots and expose the configuration of these Bots to end users it's all open source um it can be used with open AI it can be us\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].page_content[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3c5b72-0632-481a-8c24-21c237f862fa",
   "metadata": {},
   "source": [
    "## Indexing documents\n",
    "Whenever we perform retrieval we need to create an index of documents that we can query. We'll use a vector store to index our documents, and we'll chunk them first to make our retrievals more concise and precise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "059c2b9a-f3a1-4f2b-9c04-70346f82a670",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_chroma.vectorstores.Chroma at 0x7f69a0f8fad0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000)\n",
    "chunked_docs = text_splitter.split_documents(docs)\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "vectorstore = Chroma.from_documents(\n",
    "    chunked_docs,\n",
    "    embeddings,\n",
    ")\n",
    "vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af21dab0-37e1-486a-8280-c61c92a585dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['002d940b-9e15-49d3-906a-25cb25c3b33c',\n",
       " '00550b12-4edb-4589-9bdb-86675fd615f0',\n",
       " '01c5d078-4d93-40c1-b809-5081508efd7f',\n",
       " '02056be9-da67-4a3e-a1e8-827a65f2abb4',\n",
       " '0252d917-ac62-4245-aa43-306b6a12a90f']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore.get()[\"ids\"][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd0245e-210a-4668-8046-b7884191d221",
   "metadata": {},
   "source": [
    "## Retrieval without query analysis\n",
    "We can perform similarity search on a user question directly to find chunks relevant to the question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6abf73d3-de79-4568-99fd-0d7980fa798b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build and Deploy a RAG app with Pinecone Serverless\n",
      "hi this is Lance from the Lang chain team and today we're going to be building and deploying a rag app using pine con serval list from scratch so we're going to kind of walk through all the code required to do this and I'll use these slides as kind of a guide to kind of lay the the ground work um so first what is rag so under capoy has this pretty nice visualization that shows LMS as a kernel of a new kind of operating system and of course one of the core components of our operating system is th\n"
     ]
    }
   ],
   "source": [
    "search_results = vectorstore.similarity_search(\"how do I build a RAG agent\")\n",
    "\n",
    "print(search_results[0].metadata[\"title\"])  # Title of the first search result\n",
    "print(search_results[0].page_content[:500]) # First 500 characters of the first search result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d801f1-659d-4b46-b9bc-fb8eeee2f177",
   "metadata": {},
   "source": [
    "This works pretty well! Our first result is quite relevant to the question.\n",
    "\n",
    "What if we wanted to search for results from a specific time period?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "795863bd-fec5-47aa-a26a-41c1d46cec39",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build and Deploy a RAG app with Pinecone Serverless\n",
      "2024-01-16 00:00:00\n",
      "sure what's going on of course this data sets from a third party provider we didn't actually make it ourselves um so there's possible that there's some irregularities in the data itself um but let's go back we ran our chain we can see our answer you know it looks sane we can check our chain here so again here's the retrieve chunks from from our serverless index we can go look here and they're all plumbed into our prompt so that's pretty cool and here's our answer now I noted that the retriev chu\n"
     ]
    }
   ],
   "source": [
    "search_results = vectorstore.similarity_search(\"videos on RAG published in 2023\")\n",
    "\n",
    "print(search_results[0].metadata[\"title\"])\n",
    "print(search_results[0].metadata[\"publish_date\"])\n",
    "print(search_results[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2ba27a-f6b6-4684-8fd6-1324d62de2ab",
   "metadata": {},
   "source": [
    "Our first result is from 2024 (despite us asking for videos from 2023), and not very relevant to the input. Since we're just searching against document contents, there's no way for the results to be filtered on any document attributes.\n",
    "\n",
    "This is just one failure mode that can arise. Let's now take a look at how a basic form of query analysis can fix it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f36d07a-ed24-421e-b151-4f1bab31152a",
   "metadata": {},
   "source": [
    "## Query analysis\n",
    "We can use query analysis to improve the results of retrieval. This will involve defining a query schema that contains some date filters and use a function-calling model to convert a user question into a structured queries.\n",
    "\n",
    "### Query schema\n",
    "In this case we'll have explicit min and max attributes for publication date so that it can be filtered on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c52d15e9-b635-4712-baff-d0c063d40230",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "class Search(BaseModel):\n",
    "    \"\"\"Search over a database of tutorial videos about a software library.\"\"\"\n",
    "\n",
    "    query: str = Field(\n",
    "        ...,\n",
    "        description=\"Similarity search query applied to video transcripts.\",\n",
    "    )\n",
    "    publish_year: Optional[int] = Field(None, description=\"Year video was published\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3e86bf-1c14-470b-95c6-51861f7ab5f1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Query generation\n",
    "To convert user questions to structured queries we'll make use of OpenAI's tool-calling API. Specifically we'll use the new [ChatModel.with_structured_output()](https://python.langchain.com/v0.2/docs/how_to/structured_output/) constructor to handle passing the schema to the model and parsing the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4464e3d9-6bc6-4431-a156-be296da24add",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "system = \"\"\"You are an expert at converting user questions into database queries. \\\n",
    "You have access to a database of tutorial videos about a software library for building LLM-powered applications. \\\n",
    "Given a question, return a list of database queries optimized to retrieve the most relevant results.\n",
    "\n",
    "If there are acronyms or words you are not familiar with, do not try to rephrase them.\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "structured_llm = llm.with_structured_output(Search) # Pass the Search Model into the LLM\n",
    "query_analyzer = {\"question\": RunnablePassthrough()} \\\n",
    "    | prompt \\\n",
    "    | structured_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346e8667-e9c3-4088-8f94-2fef359c47f1",
   "metadata": {},
   "source": [
    "Let's see what queries our analyzer generates for the questions we searched earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "63cea198-6d2e-43ee-81fd-2ba3054bb559",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Search(query='build RAG agent', publish_year=None)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_analyzer.invoke(\"how do I build a RAG agent\") # Observe the publish_year param in Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "11b5be70-b269-4cc3-b9af-8c492c7649da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Search(query='RAG', publish_year=2023)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_analyzer.invoke(\"videos on RAG published in 2023\") # Observe the publish_year param in Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1248748-b685-47c0-9fe1-fcb50318c922",
   "metadata": {},
   "source": [
    "## Retrieval with query analysis\n",
    "Our query analysis looks pretty good; now let's try using our generated queries to actually perform retrieval.\n",
    "\n",
    "Note: in our example, we specified `tool_choice=\"Search\"`. This will force the LLM to call one - and only one - tool, meaning that we will always have one optimized query to look up. Note that this is not always the case - see other guides for how to deal with situations when no - or multiple - optmized queries are returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "95295f0b-bdd7-4106-b27a-53498f1282df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def retrieval(search: Search) -> List[Document]:\n",
    "    if search.publish_year is not None:\n",
    "        # This is syntax specific to Chroma,\n",
    "        # the vector database we are using.\n",
    "        _filter = {\"publish_year\": {\"$eq\": search.publish_year}} # Formulate the Chroma filter based on the publish_year in Seach\n",
    "    else:\n",
    "        _filter = None\n",
    "    return vectorstore.similarity_search(search.query, filter=_filter)\n",
    "\n",
    "retrieval_chain = query_analyzer | retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d36085-a694-4a92-bbac-62da061a530f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 for LLM",
   "language": "python",
   "name": "llm-python-3-11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
